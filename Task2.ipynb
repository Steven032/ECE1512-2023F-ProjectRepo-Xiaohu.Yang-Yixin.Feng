{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steven032/ECE1512-2023F-ProjectRepo-Xiaohu.Yang-Yixin.Feng/blob/main/Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WYMfvCNPwpm"
      },
      "source": [
        "# Project A: Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference\n",
        "\n",
        "https://keras.io/guides/transfer_learning/\\\n",
        "\n",
        "https://keras.io/examples/vision/knowledge_distillation/\n",
        "\n",
        "https://keras.io/guides/training_with_built_in_methods/\n",
        "\n",
        "https://keras.io/guides/transfer_learning/"
      ],
      "metadata": {
        "id": "LxsudI4i87UZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA8ppgB2P0aJ"
      },
      "outputs": [],
      "source": [
        "#tf.config.run_functions_eagerly(True)\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import Union\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from keras.models import load_model\n",
        "import matplotlib. image as image\n",
        "from matplotlib import pyplot as plt\n",
        "from random import random\n",
        "\n",
        "seed_value = 42\n",
        "\n",
        "tf.random.set_seed(seed_value)\n",
        "np.random.seed(seed_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2EFLQROP2R7"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/images.zip"
      ],
      "metadata": {
        "id": "Qq1NtkCycfb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-DHqMWIHEqe"
      },
      "outputs": [],
      "source": [
        "def create_directories(base_path, sub_folders):\n",
        "\n",
        "    if not tf.io.gfile.exists(base_path):\n",
        "        tf.io.gfile.mkdir(base_path)\n",
        "\n",
        "    for folder in sub_folders:\n",
        "        folder_path = f\"{base_path}/{folder}\"\n",
        "        if not tf.io.gfile.exists(folder_path):\n",
        "            tf.io.gfile.mkdir(folder_path)\n",
        "\n",
        "base_paths = ['/content/train', '/content/test', '/content/augmentation']\n",
        "\n",
        "sub_folders = ['HP', 'SSA']\n",
        "\n",
        "for path in base_paths:\n",
        "    create_directories(path, sub_folders)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2se7TALGHd7M"
      },
      "outputs": [],
      "source": [
        "path = '/content/annotations.csv'\n",
        "df = pd.read_csv(path)\n",
        "csv = dict(df)\n",
        "image_num = len(csv.get('Image Name'))\n",
        "\n",
        "\n",
        "for i in range(image_num):\n",
        "  if csv.get('Partition')[i] == \"train\":\n",
        "    if csv.get('Majority Vote Label')[i] == \"HP\":\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      dst = '/content/train/HP/' + csv.get('Image Name')[i]\n",
        "      tf.io.gfile.copy(src, dst, overwrite=True)\n",
        "    else:\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      dst = '/content/train/SSA/' + csv.get('Image Name')[i]\n",
        "      tf.io.gfile.copy(src, dst, overwrite=True)\n",
        "  else:\n",
        "    if csv.get('Majority Vote Label')[i] == \"HP\":\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      dst = '/content/test/HP/' + csv.get('Image Name')[i]\n",
        "      tf.io.gfile.copy(src, dst, overwrite=True)\n",
        "    else:\n",
        "      src = '/content/images/' + csv.get('Image Name')[i]\n",
        "      dst = '/content/test/SSA/' + csv.get('Image Name')[i]\n",
        "      tf.io.gfile.copy(src, dst, overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "8BV_t3mX8tLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/annotations.csv'\n",
        "df = pd.read_csv(path)\n",
        "csv = dict(df)\n",
        "image_num = len(csv.get('Image Name'))\n",
        "\n",
        "for i in range(image_num):\n",
        "    label = csv.get('Majority Vote Label')[i]\n",
        "    src = '/content/images/' + csv.get('Image Name')[i]\n",
        "    img = image.imread(src)\n",
        "    img_aug = tf.image.rot90(tf.image.random_flip_up_down(img), k=round(random()*3))\n",
        "\n",
        "    base_name = os.path.splitext(csv.get('Image Name')[i])[0]\n",
        "    extension = os.path.splitext(csv.get('Image Name')[i])[1]\n",
        "    augmented_name = base_name + '_aug' + extension\n",
        "\n",
        "    if label == \"HP\":\n",
        "        # Save augmented image\n",
        "        tf.keras.utils.save_img('/content/augmentation/HP/' + augmented_name, img_aug, data_format=None, file_format=None, scale=True)\n",
        "        # Save original image\n",
        "        tf.keras.utils.save_img('/content/augmentation/HP/' + csv.get('Image Name')[i], img, data_format=None, file_format=None, scale=True)\n",
        "    elif label == \"SSA\":\n",
        "        # Save augmented image\n",
        "        tf.keras.utils.save_img('/content/augmentation/SSA/' + augmented_name, img_aug, data_format=None, file_format=None, scale=True)\n",
        "        # Save original image\n",
        "        tf.keras.utils.save_img('/content/augmentation/SSA/' + csv.get('Image Name')[i], img, data_format=None, file_format=None, scale=True)\n"
      ],
      "metadata": {
        "id": "IMBwkrjbw5hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynByMG_UP4A4"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/augmentation'\n",
        "\n",
        "BATCH_SIZE=32\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,shuffle=True,batch_size=BATCH_SIZE,image_size=(224,224))\n",
        "\n",
        "\n",
        "\n",
        "test_dir = '/content/test'\n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(test_dir,shuffle=True,batch_size=BATCH_SIZE,image_size=(224,224))\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pDr3-PnIWZE"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.float32), tf.one_hot(y, 2)))\n",
        "test_dataset = test_dataset.map(lambda x, y: (tf.cast(x, tf.float32), tf.one_hot(y, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss unctions and evaluate function\n",
        "\n"
      ],
      "metadata": {
        "id": "K76M0vBFBoD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "ALPHA = 0.5\n",
        "TEMPERATURE = 4\n",
        "\n",
        "\n",
        "def compute_student_loss_withoutKD(images, labels):\n",
        "  subclass_logits = student_model(images, training=True)\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
        "  return cross_entropy_loss_value\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "\n",
        "def compute_student_loss(images, labels):\n",
        "  student_subclass_logits = student_model(images, training=True)\n",
        "  teacher_subclass_logits = teacher_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits,student_subclass_logits,DISTILLATION_TEMPERATURE)\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels, student_subclass_logits)\n",
        "  loss= ALPHA*distillation_loss_value + (1-ALPHA)* cross_entropy_loss_value\n",
        "  return loss\n",
        "\n",
        "def compute_num_correct(model, images, labels):\n",
        "  class_logits = model(images, training=False)\n",
        "  value= tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
        "  return value\n",
        "\n",
        "def compute_predictions(model, images):\n",
        "    logits = model(images, training=False)\n",
        "    return tf.argmax(logits, axis=1)\n",
        "\n",
        "def compute_true_labels(labels):\n",
        "    return tf.argmax(labels, axis=1)\n",
        "\n",
        "def compute_teacher_loss(images, labels):\n",
        "  subclass_logits = teacher_model(images, training=True)\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
        "  return cross_entropy_loss_value\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn):\n",
        "    time_matrix = []\n",
        "    f1_scores_label_0 = []  # Store F1 scores for label 0\n",
        "    f1_scores_label_1 = []  # Store F1 scores for label 1\n",
        "    macro_f1 = []\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "        start_time = time.time()\n",
        "        print('Epoch {}: '.format(epoch), end='')\n",
        "        for images, labels in train_dataset:\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss_value = compute_loss_fn(images, labels)\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        for images, labels in test_dataset:\n",
        "            predictions = compute_predictions(model, images)\n",
        "            true_labels = compute_true_labels(labels)\n",
        "\n",
        "            all_predictions.append(predictions)\n",
        "            all_labels.append(true_labels)\n",
        "\n",
        "        all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "        all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        epoch_confusion_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "        epoch_f1_individual = f1_score(all_labels, all_predictions, average=None)\n",
        "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "        print(\"macro F1 Scores:\", f1_macro)\n",
        "        print(epoch_confusion_matrix)\n",
        "        print(\"F1 Scores for each label:\", epoch_f1_individual)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        time_matrix.append(epoch_time)\n",
        "\n",
        "        f1_scores_label_0.append(epoch_f1_individual[0])\n",
        "        f1_scores_label_1.append(epoch_f1_individual[1])\n",
        "        macro_f1.append(f1_macro)\n",
        "\n",
        "        print(\"Total time per epoch: {0:.2f} seconds\".format(epoch_time))\n",
        "\n",
        "    total_time = np.sum(time_matrix)\n",
        "    print(\"Total time of training this model: {0:.2f} seconds\".format(total_time))\n",
        "\n",
        "    return f1_scores_label_0, f1_scores_label_1,macro_f1"
      ],
      "metadata": {
        "id": "nCogclQ0NVW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Evalute"
      ],
      "metadata": {
        "id": "QAfqxzNqtju-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-trained ResNet50V2**"
      ],
      "metadata": {
        "id": "UrMTuxTzxiLT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xYQjXire2OZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "#Reference: https://keras.io/guides/transfer_learning/\n",
        "input_size = (224,224,3)\n",
        "teacher_resnet_50 = tf.keras.applications.resnet_v2.ResNet50V2(input_shape=input_size,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "\n",
        "teacher_resnet_50.trainable=False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = teacher_resnet_50(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "teacher_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "teacher_model.compile(optimizer=optimizer)\n",
        "\n",
        "teacher_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iZF0woQe2GE"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "h1, h2,h3 = train_and_evaluate(teacher_model, compute_teacher_loss)\n",
        "\n",
        "f1_1 = h1\n",
        "f1_2 = h2\n",
        "f1_3 = h3\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(f1_1, label='F1: HP',linestyle='--', marker='o')\n",
        "plt.plot(f1_2, label='F1: SSA',linestyle='--', marker='o')\n",
        "plt.plot(f1_3, label='micro F1 ',linestyle='--', marker='o')\n",
        "\n",
        "plt.ylabel('F1')\n",
        "plt.ylim([0, 1])\n",
        "\n",
        "plt.legend()\n",
        "plt.title('F1 of Teacher Model')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fine-tune teacher model**\n"
      ],
      "metadata": {
        "id": "dW5VfYpck9cG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmRk3MJ0e1_A"
      },
      "outputs": [],
      "source": [
        "teacher_model.trainable = True\n",
        "\n",
        "for layer in teacher_model.layers:\n",
        "  layer.trainable = False\n",
        "for layer in teacher_model.layers[-4:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "optimizer=tf.keras.optimizers.Adam(1e-5)\n",
        "teacher_model.compile(optimizer=optimizer)\n",
        "\n",
        "teacher_model.summary()\n",
        "\n",
        "NUM_EPOCHS = 25\n",
        "h1_finetune, h2_finetune,h3_finetune = train_and_evaluate(teacher_model, compute_teacher_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save the fine-tuned teacher model"
      ],
      "metadata": {
        "id": "7AdDqp7cE0pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model.save('/content/teacher_model.h5')"
      ],
      "metadata": {
        "id": "CtKARFhdBTqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization for teacher model f1 score before and after fine-tune"
      ],
      "metadata": {
        "id": "VHkZMG8C9KbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_epoch=10\n",
        "f1_1 += h1_finetune\n",
        "f1_2 += h2_finetune\n",
        "f1_3 += h3_finetune\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "ax.plot(f1_1, linestyle='--', marker='o', label='F1 : HP')\n",
        "ax.plot(f1_2, linestyle='--', marker='o', label='F1 : SSA')\n",
        "ax.plot(f1_3, linestyle='--', marker='o', label='macro F1')\n",
        "\n",
        "ax.axvline(x=initial_epoch, linestyle='--', label='Fine Tuning')\n",
        "\n",
        "ax.set_title('Accuracy of Teacher Model')\n",
        "ax.set_ylabel('F1')\n",
        "ax.set_ylim([0, 1])\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cXbBoolTMpkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and evaluate student model"
      ],
      "metadata": {
        "id": "BtNc88YW06y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference: https://keras.io/guides/transfer_learning/\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "\n",
        "input_size = (224,224,3)\n",
        "student_base_model= tf.keras.applications.MobileNetV2(input_shape=input_size,\n",
        "                                              include_top=False,\n",
        "                                              weights='imagenet')\n",
        "\n",
        "student_base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = student_base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = Dense(200, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "student_model = tf.keras.Model(inputs, outputs)\n",
        "student_model.summary()\n",
        "\n",
        "student_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3))"
      ],
      "metadata": {
        "id": "5lkOsui-eCJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the student model from scratch with 10 epochs and fine-tune with 25 epochs**"
      ],
      "metadata": {
        "id": "Z-5lBJ2CFCN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cdJCkbnf_01"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS= 10\n",
        "optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "student_f1_1, student_f1_2,student_f1_3= train_and_evaluate(student_model,compute_student_loss_withoutKD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lDhppFSf_sf"
      },
      "outputs": [],
      "source": [
        "def fine_tune(model):\n",
        "  model.trainable = True\n",
        "  for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "  for layer in model.layers[-2:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "  optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n",
        "  model.compile(optimizer=optimizer)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsv0U2ocf_qF"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS= 25\n",
        "student_model = fine_tune(student_model)\n",
        "\n",
        "student_f1_1_fine, student_f1_2_fine,student_f1_3_fine = train_and_evaluate(student_model,compute_student_loss_withoutKD)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**save the fine tuned student model**"
      ],
      "metadata": {
        "id": "rfkTWmNiFNKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_model.save('/content/student_model.h5')"
      ],
      "metadata": {
        "id": "k0D7YNtPGSQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the student Model with knowledge Distillation**"
      ],
      "metadata": {
        "id": "ar8bCf-xFVaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (224,224,3)\n",
        "student_base_model= tf.keras.applications.MobileNetV2(input_shape=input_size,\n",
        "                                              include_top=False,\n",
        "                                              weights='imagenet')\n",
        "\n",
        "student_base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = student_base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "student_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "student_model.compile(optimizer=optimizer)\n"
      ],
      "metadata": {
        "id": "JvE8-FWcZABF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model.summary()"
      ],
      "metadata": {
        "id": "2ZHK8UGfzhTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reference: https://keras.io/guides/transfer_learning/\n",
        "input_size = (224,224,3)\n",
        "teacher_resnet_50 = tf.keras.applications.resnet_v2.ResNet50V2(input_shape=input_size,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "\n",
        "teacher_resnet_50.trainable=False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = teacher_resnet_50(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "teacher_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "teacher_model.compile(optimizer=optimizer)\n",
        "\n",
        "teacher_model.summary()"
      ],
      "metadata": {
        "id": "uE81u8T3eq2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model = tf.keras.models.load_model(\"teacher_model.h5\")"
      ],
      "metadata": {
        "id": "_DfeIzQJesf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_model = tf.keras.models.load_model(\"student_model.h5\")\n",
        "#student_model = fine_tune(student_model)\n",
        "student_model.summary()"
      ],
      "metadata": {
        "id": "rCzyxrI9CPTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALPHA = 0.4\n",
        "DISTILLATION_TEMPERATURE = 64\n",
        "NUM_EPOCHS= 25\n",
        "optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
        "student_model.summary()"
      ],
      "metadata": {
        "id": "EvUVmNUBpQRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "student_f1_1_fine_KD, student_f1_2_fine_KD,student_f1_3_fine_KD = train_and_evaluate(student_model,compute_student_loss)"
      ],
      "metadata": {
        "id": "3RPFTl4fCk4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm6JSxQmf_nV"
      },
      "outputs": [],
      "source": [
        "f1_1_student = student_f1_1 + student_f1_1_fine + student_f1_1_fine_KD\n",
        "f1_2_student = student_f1_2 + student_f1_2_fine + student_f1_2_fine_KD\n",
        "f1_3_student = student_f1_3 + student_f1_3_fine + student_f1_3_fine_KD\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "ax.plot(f1_1_student, linestyle='--', marker='o', label='F1 Score: HP')\n",
        "ax.plot(f1_2_student, linestyle='--', marker='o', label='F1 Score: SSA')\n",
        "ax.plot(f1_3_student, linestyle='--', marker='o', label='micro F1')\n",
        "\n",
        "ax.axvline(x=initial_epoch, color='red', linestyle='--', label='Start Fine Tuning')\n",
        "ax.axvline(x=35, color='gray', linestyle='--', label='Start KD')\n",
        "\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_ylim([0, 1])\n",
        "ax.legend(loc='lower right')\n",
        "ax.set_title('F1 score of Student Model')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1 Score with different Temperature"
      ],
      "metadata": {
        "id": "Yggm49QHZtRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG0DERwaf_kL"
      },
      "outputs": [],
      "source": [
        "Temperatures = [1,2,4,16,32,64]\n",
        "\n",
        "ALPHA=0.5\n",
        "student_f1_scores_1 = []\n",
        "student_f1_scores_2 = []\n",
        "student_f1_scores_3 = []\n",
        "\n",
        "NUM_EPOCHS= 12\n",
        "\n",
        "\n",
        "for t in Temperatures:\n",
        "  DISTILLATION_TEMPERATURE = t\n",
        "  tf.keras.backend.clear_session()\n",
        "  input_size = (224,224,3)\n",
        "  student_base_model= tf.keras.applications.MobileNetV2(input_shape=input_size,\n",
        "                                                include_top=False,\n",
        "                                                weights='imagenet')\n",
        "\n",
        "  student_base_model.trainable = False\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(224,224,3))\n",
        "  x = student_base_model(inputs, training=False)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  outputs = tf.keras.layers.Dense(2)(x)\n",
        "  student_model = tf.keras.Model(inputs, outputs)\n",
        "  student_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3))\n",
        "  optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
        "  student_model = fine_tune(student_model)\n",
        "  student_model = load_model('/content/student_model.h5')\n",
        "  f1_fine_1_KD,f1_fine_2_KD,f1_fine_3_KD = train_and_evaluate(student_model,compute_student_loss)\n",
        "  student_f1_scores_1.append(f1_fine_1_KD)\n",
        "  student_f1_scores_2.append(f1_fine_2_KD)\n",
        "  student_f1_scores_3.append(f1_fine_3_KD)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5qxT5vHfdOX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "for i in range(len(student_f1_scores_1)):\n",
        "  plt.plot(student_f1_scores_1[i],linestyle='--', marker='o', label='Temperature={}'.format(Temperatures[i]))\n",
        "  plt.ylabel('F1 Score')\n",
        "  plt.legend(loc='lower left')\n",
        "  plt.title('Student Model F1 Scores for class HP With Temperatures[1,2,4,16,32,64]')\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "for i in range(len(student_f1_scores_2)):\n",
        "  plt.plot(student_f1_scores_2[i],linestyle='--', marker='o', label='Temperature={}'.format(Temperatures[i]))\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.ylabel('F1 Score')\n",
        "  plt.title('Student Model F1 Scores for class SSA With Temperatures[1,2,4,16,32,64]')\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "for i in range(len(student_f1_scores_3)):\n",
        "  plt.plot(student_f1_scores_3[i],linestyle='--', marker='o', label='Temperature={}'.format(Temperatures[i]))\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.ylabel('F1 Score')\n",
        "  plt.title('Student Model F1 Scores With Temperatures[1,2,4,16,32,64]')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# implement with state of art model in paper 2 and 9"
      ],
      "metadata": {
        "id": "OwaIRccuvNL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## paper 2 with early stopping knowledge distillation"
      ],
      "metadata": {
        "id": "U6sb18i5vY-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_resnet_50 = tf.keras.applications.resnet_v2.ResNet50V2(\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "\n",
        "teacher_resnet_50.trainable=False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = teacher_resnet_50(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(dtype='float32')(x)\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "teacher_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "teacher_model.compile(optimizer=optimizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "GcYHQfx-zk7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (224,224,3)\n",
        "student_base_model= tf.keras.applications.MobileNetV2(input_shape=input_size,\n",
        "                                              include_top=False,\n",
        "                                              weights='imagenet')\n",
        "\n",
        "student_base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = student_base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = Dense(200, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "student_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "student_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3))"
      ],
      "metadata": {
        "id": "9nbffHnrzm8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model = tf.keras.models.load_model(\"teacher_model.h5\")\n",
        "student_model = tf.keras.models.load_model(\"student_model.h5\")"
      ],
      "metadata": {
        "id": "9bkpz8GTyCr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "ALPHA = 0.5\n",
        "TEMPERATURE = 64\n",
        "\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
        "\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "def compute_student_loss_ES(images, labels,epochs):\n",
        "  student_subclass_logits = student_model(images, training=True)\n",
        "  teacher_subclass_logits = teacher_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits,student_subclass_logits,DISTILLATION_TEMPERATURE)\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels, student_subclass_logits)\n",
        "  if epochs <=10:\n",
        "    loss = ALPHA*distillation_loss_value+(1-ALPHA)*cross_entropy_loss_value\n",
        "  else:\n",
        "    cross_entropy_loss_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels,student_subclass_logits))\n",
        "    loss = cross_entropy_loss_value\n",
        "  return loss\n",
        "\n",
        "\n",
        "def compute_num_correct(model, images, labels):\n",
        "  class_logits = model(images, training=False)\n",
        "  value= tf.reduce_sum(\n",
        "      tf.cast(tf.math.equal(tf.argmax(class_logits, -1), tf.argmax(labels, -1)),\n",
        "              tf.float32)), tf.argmax(class_logits, -1), tf.argmax(labels, -1)\n",
        "  return value\n",
        "\n",
        "def compute_predictions(model, images):\n",
        "    logits = model(images, training=False)\n",
        "    return tf.argmax(logits, axis=1)\n",
        "\n",
        "def compute_true_labels(labels):\n",
        "    return tf.argmax(labels, axis=1)\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn):\n",
        "    time_matrix = []\n",
        "    f1_scores_label_0 = []  # Store F1 scores for label 0\n",
        "    f1_scores_label_1 = []  # Store F1 scores for label 1\n",
        "    macro_f1 = []\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "        start_time = time.time()\n",
        "        print('Epoch {}: '.format(epoch), end='')\n",
        "        for images, labels in train_dataset:\n",
        "            with tf.GradientTape() as tape:\n",
        "                  loss_value = compute_loss_fn(images,labels,epoch)\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        for images, labels in test_dataset:\n",
        "            predictions = compute_predictions(model, images)\n",
        "            true_labels = compute_true_labels(labels)\n",
        "\n",
        "            all_predictions.append(predictions)\n",
        "            all_labels.append(true_labels)\n",
        "\n",
        "        all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "        all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        epoch_confusion_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "        epoch_f1_individual = f1_score(all_labels, all_predictions, average=None)\n",
        "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "        print(\"macro F1 Scores:\", f1_macro)\n",
        "        print(epoch_confusion_matrix)\n",
        "        print(\"F1 Scores for each label:\", epoch_f1_individual)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        time_matrix.append(epoch_time)\n",
        "\n",
        "        f1_scores_label_0.append(epoch_f1_individual[0])\n",
        "        f1_scores_label_1.append(epoch_f1_individual[1])\n",
        "        macro_f1.append(f1_macro)\n",
        "\n",
        "        print(\"Total time per epoch: {0:.2f} seconds\".format(epoch_time))\n",
        "\n",
        "    total_time = np.sum(time_matrix)\n",
        "    print(\"Total time of training this model: {0:.2f} seconds\".format(total_time))\n",
        "\n",
        "    return f1_scores_label_0, f1_scores_label_1,macro_f1"
      ],
      "metadata": {
        "id": "1aP4rStSvWj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS= 25\n",
        "train_and_evaluate(student_model, compute_student_loss_ES)"
      ],
      "metadata": {
        "id": "xdLxRjK4yUoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## paper 9 with teacher assistant model"
      ],
      "metadata": {
        "id": "LleZUdo80BvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TA model is choosen as EffiecientNetB0, which is a model designed to be both smaller in size and more efficient in terms of performance and has a model size between ResNetV2 and MobileNetV2."
      ],
      "metadata": {
        "id": "EyFsnkDpHnlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the EfficientNetB0 model as the TA model\n",
        "teacher_efficientnet = tf.keras.applications.EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "teacher_efficientnet.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = teacher_efficientnet(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "ta_model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the TA model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "ta_model.compile(optimizer=optimizer)\n"
      ],
      "metadata": {
        "id": "JGhL7Nll0IPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (224,224,3)\n",
        "student_base_model= tf.keras.applications.MobileNetV2(input_shape=input_size,\n",
        "                                              include_top=False,\n",
        "                                              weights='imagenet')\n",
        "\n",
        "student_base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = student_base_model(inputs, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = Dense(200, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(2)(x)\n",
        "student_model = tf.keras.Model(inputs, outputs)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "student_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3))"
      ],
      "metadata": {
        "id": "iPxkgzr1Bu19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "ALPHA = 0.5\n",
        "TEMPERATURE = 4\n",
        "\n",
        "def compute_ta_loss_finetune(images, labels):\n",
        "  subclass_logits = ta_model(images, training=True)\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=subclass_logits)\n",
        "  return cross_entropy_loss_value\n",
        "\n",
        "def distillation_loss(teacher_logits: tf.Tensor, student_logits: tf.Tensor,\n",
        "                      temperature: Union[float, tf.Tensor]):\n",
        "  soft_targets = tf.nn.softmax(teacher_logits / temperature)\n",
        "\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits(\n",
        "          soft_targets, student_logits / temperature)) * temperature ** 2\n",
        "\n",
        "def compute_ta_loss(images, labels):\n",
        "  ta_subclass_logits = ta_model(images, training=True)\n",
        "  teacher_subclass_logits = teacher_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits,ta_subclass_logits,DISTILLATION_TEMPERATURE)\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels, ta_subclass_logits)\n",
        "  loss= ALPHA*distillation_loss_value + (1-ALPHA)* cross_entropy_loss_value\n",
        "  return loss\n",
        "\n",
        "\n",
        "def compute_student_loss(images, labels):\n",
        "  student_subclass_logits = student_model(images, training=True)\n",
        "  teacher_subclass_logits = ta_model(images, training=False)\n",
        "  distillation_loss_value = distillation_loss(teacher_subclass_logits,student_subclass_logits,DISTILLATION_TEMPERATURE)\n",
        "  cross_entropy_loss_value = tf.nn.softmax_cross_entropy_with_logits(labels, student_subclass_logits)\n",
        "  loss= ALPHA*distillation_loss_value + (1-ALPHA)* cross_entropy_loss_value\n",
        "  return loss\n",
        "\n",
        "\n",
        "def compute_predictions(model, images):\n",
        "    logits = model(images, training=False)\n",
        "    return tf.argmax(logits, axis=1)\n",
        "\n",
        "def compute_true_labels(labels):\n",
        "    return tf.argmax(labels, axis=1)\n",
        "\n",
        "def train_and_evaluate(model, compute_loss_fn):\n",
        "    time_matrix = []\n",
        "    f1_scores_label_0 = []  # Store F1 scores for label 0\n",
        "    f1_scores_label_1 = []  # Store F1 scores for label 1\n",
        "    macro_f1 = []\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "\n",
        "        start_time = time.time()\n",
        "        print('Epoch {}: '.format(epoch), end='')\n",
        "        for images, labels in train_dataset:\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss_value = compute_loss_fn(images, labels)\n",
        "            #print(loss_value)\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        for images, labels in test_dataset:\n",
        "            predictions = compute_predictions(model, images)\n",
        "            true_labels = compute_true_labels(labels)\n",
        "\n",
        "            all_predictions.append(predictions)\n",
        "            all_labels.append(true_labels)\n",
        "\n",
        "        all_predictions = np.concatenate(all_predictions, axis=0)\n",
        "        all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        epoch_confusion_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "        epoch_f1_individual = f1_score(all_labels, all_predictions, average=None)\n",
        "        f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "        print(\"macro F1 Scores:\", f1_macro)\n",
        "        print(epoch_confusion_matrix)\n",
        "        print(\"F1 Scores for each label:\", epoch_f1_individual)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        time_matrix.append(epoch_time)\n",
        "\n",
        "        f1_scores_label_0.append(epoch_f1_individual[0])\n",
        "        f1_scores_label_1.append(epoch_f1_individual[1])\n",
        "        macro_f1.append(f1_macro)\n",
        "\n",
        "        print(\"Total time per epoch: {0:.2f} seconds\".format(epoch_time))\n",
        "\n",
        "    total_time = np.sum(time_matrix)\n",
        "    print(\"Total time of training this model: {0:.2f} seconds\".format(total_time))\n",
        "\n",
        "    return f1_scores_label_0, f1_scores_label_1,macro_f1"
      ],
      "metadata": {
        "id": "KpvB9OIA2EP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model = tf.keras.models.load_model(\"teacher_model.h5\")\n",
        "student_model = tf.keras.models.load_model(\"student_model.h5\")"
      ],
      "metadata": {
        "id": "GWj6gSV_1VYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fine tune TA**"
      ],
      "metadata": {
        "id": "B02UD7KU437j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ta_model.trainable = True\n",
        "\n",
        "for layer in ta_model.layers:\n",
        "  layer.trainable = False\n",
        "for layer in ta_model.layers[-3:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "optimizer=tf.keras.optimizers.Adam(1e-3)\n",
        "ta_model.compile(optimizer=optimizer)\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 25\n",
        "train_and_evaluate(ta_model, compute_ta_loss_finetune)"
      ],
      "metadata": {
        "id": "hF8BhHAe1hCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KD ta model**"
      ],
      "metadata": {
        "id": "VkhJ3cHW47Id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(ta_model, compute_ta_loss)"
      ],
      "metadata": {
        "id": "3g7t6kH74wWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KD student model**"
      ],
      "metadata": {
        "id": "Q_zspuoC49PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(student_model, compute_student_loss)"
      ],
      "metadata": {
        "id": "_fJbYgZi4_ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# flops"
      ],
      "metadata": {
        "id": "4_TD2MRwyS8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_flops(teacher_model):\n",
        "    session = tf.compat.v1.Session()\n",
        "    graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "    with graph.as_default():\n",
        "        with session.as_default():\n",
        "            model = tf.keras.models.clone_model(teacher_model)\n",
        "            run_meta = tf.compat.v1.RunMetadata()\n",
        "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
        "\n",
        "            flops = tf.compat.v1.profiler.profile(graph=graph,\n",
        "                                                  run_meta=run_meta, cmd='op', options=opts)\n",
        "            return flops.total_float_ops\n",
        "\n",
        "print(f\"The number of FLOPs of the teacher model: {get_flops(teacher_model)}\")\n",
        "print(f\"The number of FLOPs of the student model: {get_flops(student_model)}\")"
      ],
      "metadata": {
        "id": "cWuYEzMoEvhL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}